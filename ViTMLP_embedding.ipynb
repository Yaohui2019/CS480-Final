{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":81655,"databundleVersionId":8915386,"sourceType":"competition"},{"sourceId":9159549,"sourceType":"datasetVersion","datasetId":5533689},{"sourceId":9160506,"sourceType":"datasetVersion","datasetId":5533590}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom catboost import Pool, CatBoostRegressor\nfrom torchvision import transforms\nfrom torch import nn\n\nimport imageio.v3 as imageio\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom transformers import ViTModel\ntqdm.pandas()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-12T18:02:23.388921Z","iopub.execute_input":"2024-08-12T18:02:23.389296Z","iopub.status.idle":"2024-08-12T18:02:23.396888Z","shell.execute_reply.started":"2024-08-12T18:02:23.389257Z","shell.execute_reply":"2024-08-12T18:02:23.395798Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Config():\n    TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n    # Dataset\n    RECOMPUTE_DATAFRAMES_TRAIN = True\n    RECOMPUTE_DATAFRAMES_TEST = True\n    RECOMPUTE_IMAGE_EMBEDDINGS = True\n    N_VAL_SAMPLES0 = 4096\n    # Others\n    SEED = 20898485\n    DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    \ndef seed_everything(seed: int):    \n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n        \nCONFIG = Config()\nseed_everything(CONFIG.SEED)\nCONFIG.DEVICE","metadata":{"execution":{"iopub.status.busy":"2024-08-12T18:02:25.854260Z","iopub.execute_input":"2024-08-12T18:02:25.854890Z","iopub.status.idle":"2024-08-12T18:02:25.864796Z","shell.execute_reply.started":"2024-08-12T18:02:25.854855Z","shell.execute_reply":"2024-08-12T18:02:25.863962Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'cuda:0'"},"metadata":{}}]},{"cell_type":"code","source":"#Read Data\ndata = pd.read_csv('/kaggle/input/cs-480-2024-spring/data/train.csv')\ndata['file_path'] = data['id'].apply(lambda s: f'/kaggle/input/cs-480-2024-spring/data/train_images/{s}.jpeg')\n\ndata_test = pd.read_csv('/kaggle/input/cs-480-2024-spring/data/test.csv')\ndata_test['file_path'] = data_test['id'].apply(lambda s: f'/kaggle/input/cs-480-2024-spring/data/test_images/{s}.jpeg')\ntest = data_test\n\n\n#Transform the data\nfor target in CONFIG.TARGET_COLUMNS:\n    v = data[target].values\n    data[target] = np.log10(v)\n\nSCALER = StandardScaler()\ndata[CONFIG.TARGET_COLUMNS] = SCALER.fit_transform(data[CONFIG.TARGET_COLUMNS])\n\n#Train Test Split\nCONFIG.FEATURE_COLUMNS = data_test.columns.values[1:-2]\ntrain, val = train_test_split(data, test_size=CONFIG.N_VAL_SAMPLES0, shuffle=True, random_state=CONFIG.SEED)\ntrain = train.reset_index(drop=True)\nval = val.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T18:02:30.284587Z","iopub.execute_input":"2024-08-12T18:02:30.284949Z","iopub.status.idle":"2024-08-12T18:02:31.562475Z","shell.execute_reply.started":"2024-08-12T18:02:30.284919Z","shell.execute_reply":"2024-08-12T18:02:31.561227Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class ViTWithMLP(nn.Module):\n    def __init__(self, vit,hidden_dim,output_dim):\n        super(ViTWithMLP, self).__init__()\n        self.vit = vit\n        self.mlp = nn.Sequential(\n            nn.Linear(vit.config.hidden_size, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, output_dim)  # Final layer to be removed\n        )\n    \n    def forward(self, x):\n        outputs = self.vit(pixel_values=x)\n        features = outputs.last_hidden_state[:, 0, :]\n        output = self.mlp(features)\n        return output\n    \nmodel = torch.load(\"/kaggle/input/models/model.pth\")\nmodel.mlp = nn.Sequential(*list(model.mlp.children())[:-1])\nmodel.to(CONFIG.DEVICE)","metadata":{"execution":{"iopub.status.busy":"2024-08-12T18:02:53.132501Z","iopub.execute_input":"2024-08-12T18:02:53.133079Z","iopub.status.idle":"2024-08-12T18:03:02.641206Z","shell.execute_reply.started":"2024-08-12T18:02:53.133043Z","shell.execute_reply":"2024-08-12T18:03:02.640316Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"ViTWithMLP(\n  (vit): ViTModel(\n    (embeddings): ViTEmbeddings(\n      (patch_embeddings): ViTPatchEmbeddings(\n        (projection): Conv2d(3, 1024, kernel_size=(16, 16), stride=(16, 16))\n      )\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViTEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x ViTLayer(\n          (attention): ViTSdpaAttention(\n            (attention): ViTSdpaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViTSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViTIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViTOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViTPooler(\n      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (mlp): Sequential(\n    (0): Linear(in_features=1024, out_features=32, bias=True)\n    (1): ReLU()\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def get_image_embeddings(model, preprocess, batch_size, df):\n    image_embeddings = []\n    for i in tqdm(range(0, len(df), batch_size)):\n        paths = df['file_path'][i:i + batch_size]\n        image_tensor = torch.stack([preprocess(image=imageio.imread(path))['image'] for path in paths]).to(CONFIG.DEVICE)\n        with torch.no_grad():\n            curr_image_embeddings = model(image_tensor)\n        image_embeddings.extend(curr_image_embeddings.cpu().numpy())\n    return image_embeddings\n\nmodel.eval()\n# the preprocessing differs from the original code, originally it was resize + crop\n# but we lose info while cropping, so here we use only resize to 224\nMEAN = np.array([0.485, 0.456, 0.406])\nSTD = np.array([0.229, 0.224, 0.225])\n\npreprocess = A.Compose([\n        A.Resize(384, 384),\n        A.ToFloat(),\n        A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n        ToTensorV2(),\n    ])\n\nbatch_size = 64\nsuffix = 'ViTMLP'\n\ntrain_image_embeddings = get_image_embeddings(model, preprocess, batch_size, train)\nnp.save(f'train_{suffix}', np.array(train_image_embeddings))\nval_image_embeddings = get_image_embeddings(model, preprocess, batch_size, val)\nnp.save(f'val_{suffix}', np.array(val_image_embeddings))\ntest_image_embeddings = get_image_embeddings(model, preprocess, batch_size, test)\nnp.save(f'test_{suffix}', np.array(test_image_embeddings))","metadata":{"execution":{"iopub.status.busy":"2024-08-12T16:53:18.051035Z","iopub.execute_input":"2024-08-12T16:53:18.051413Z","iopub.status.idle":"2024-08-12T16:53:18.061257Z","shell.execute_reply.started":"2024-08-12T16:53:18.051386Z","shell.execute_reply":"2024-08-12T16:53:18.060235Z"},"trusted":true},"execution_count":null,"outputs":[]}]}