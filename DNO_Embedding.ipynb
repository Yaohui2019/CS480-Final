{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":81655,"databundleVersionId":8915386,"sourceType":"competition"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom catboost import Pool, CatBoostRegressor\nfrom torchvision import transforms\n\ntqdm.pandas()","metadata":{"execution":{"iopub.status.busy":"2024-08-11T05:51:58.697711Z","iopub.execute_input":"2024-08-11T05:51:58.698497Z","iopub.status.idle":"2024-08-11T05:52:04.617228Z","shell.execute_reply.started":"2024-08-11T05:51:58.698466Z","shell.execute_reply":"2024-08-11T05:52:04.616377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Config():\n    TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n    # Dataset\n    RECOMPUTE_DATAFRAMES_TRAIN = True\n    RECOMPUTE_DATAFRAMES_TEST = True\n    RECOMPUTE_IMAGE_EMBEDDINGS = True\n    N_VAL_SAMPLES0 = 4096\n    # Others\n    SEED = 20898485\n    DEVICE = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n    \ndef seed_everything(seed: int):    \n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n        \nCONFIG = Config()\nseed_everything(CONFIG.SEED)\nCONFIG.DEVICE","metadata":{"execution":{"iopub.status.busy":"2024-08-11T05:52:15.770151Z","iopub.execute_input":"2024-08-11T05:52:15.770874Z","iopub.status.idle":"2024-08-11T05:52:15.814349Z","shell.execute_reply.started":"2024-08-11T05:52:15.770830Z","shell.execute_reply":"2024-08-11T05:52:15.813427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load pickled dataframes from a public dataset; split to train-val\nif CONFIG.RECOMPUTE_DATAFRAMES_TRAIN:\n    train0 = pd.read_csv('/kaggle/input/cs-480-2024-spring/data/train.csv')\n    train0['file_path'] = train0['id'].apply(lambda s: f'/kaggle/input/cs-480-2024-spring/data/train_images/{s}.jpeg')\nelse:\n    train0 = pd.read_pickle('/kaggle/input/planttraits2024-eda-training-pub-dataset/train.pkl')\n    \nif CONFIG.RECOMPUTE_DATAFRAMES_TEST:\n    test = pd.read_csv('/kaggle/input/cs-480-2024-spring/data/test.csv')\n    test['file_path'] = test['id'].apply(lambda s: f'/kaggle/input/cs-480-2024-spring/data/test_images/{s}.jpeg')\nelse:\n    test = pd.read_pickle('/kaggle/input/planttraits2024-eda-training-pub-dataset/test.pkl')\nCONFIG.FEATURE_COLUMNS = test.columns.values[1:-2]\n\ntrain, val = train_test_split(train0, test_size=CONFIG.N_VAL_SAMPLES0, shuffle=True, random_state=CONFIG.SEED)\ntrain = train.reset_index(drop=True)\nval = val.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-11T05:53:42.423162Z","iopub.execute_input":"2024-08-11T05:53:42.423974Z","iopub.status.idle":"2024-08-11T05:53:44.383715Z","shell.execute_reply.started":"2024-08-11T05:53:42.423942Z","shell.execute_reply":"2024-08-11T05:53:44.382745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_mask(df, labels_describe_df):\n#     lower = []\n#     higher = []\n#     mask = np.empty(shape=df[CONFIG.TARGET_COLUMNS].shape, dtype=bool)\n#     for idx, t in enumerate(CONFIG.TARGET_COLUMNS):\n#         labels = df[t].values\n#         v_min, v_max = labels_describe_df.loc[t]['0.1%'], labels_describe_df.loc[t]['98%']\n#         mask[:,idx] = ((labels > v_min) & (labels < v_max))\n#     return mask.min(axis=1)\n\n# labels_describe_df = train[CONFIG.TARGET_COLUMNS].describe(percentiles=[0.001, 0.98]).round(3).T\n# # Masks\n# mask_train = get_mask(train, labels_describe_df)\n# mask_val = get_mask(val, labels_describe_df)\n# # Masked DataFrames\n# train_mask = train[mask_train].reset_index(drop=True)\n# val_mask = val[mask_val].reset_index(drop=True)\n\n# for m, subset, full in zip([train_mask, val_mask], ['train', 'val'], [train, val]):\n#     print(f'===== {subset} shape: {m.shape} =====')\n#     n_masked = len(full) - len(m)\n#     perc_masked = (n_masked / len(full)) * 100\n#     print(f'{subset} \\t| # Masked Samples: {n_masked}')\n#     print(f'{subset} \\t| % Masked Samples: {perc_masked:.3f}%')","metadata":{"execution":{"iopub.status.busy":"2024-08-11T05:53:55.691430Z","iopub.execute_input":"2024-08-11T05:53:55.691795Z","iopub.status.idle":"2024-08-11T05:53:55.780385Z","shell.execute_reply.started":"2024-08-11T05:53:55.691767Z","shell.execute_reply":"2024-08-11T05:53:55.779336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Get DINO Embedding","metadata":{}},{"cell_type":"code","source":"def get_image_embeddings_dino(model, preprocess, batch_size, df):\n    image_embeddings = []\n    for i in tqdm(range(0, len(df), batch_size)):\n        paths = df['file_path'][i:i + batch_size]\n        image_tensor = torch.stack([preprocess(Image.open(path)) for path in paths]).to(CONFIG.DEVICE)\n        with torch.no_grad():\n            curr_image_embeddings = model(image_tensor)\n        image_embeddings.extend(curr_image_embeddings.cpu().numpy())\n    return image_embeddings","metadata":{"execution":{"iopub.status.busy":"2024-08-11T05:56:56.322234Z","iopub.execute_input":"2024-08-11T05:56:56.322936Z","iopub.status.idle":"2024-08-11T05:56:56.329113Z","shell.execute_reply.started":"2024-08-11T05:56:56.322906Z","shell.execute_reply":"2024-08-11T05:56:56.328092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_reg').to(CONFIG.DEVICE)\nmodel.eval()\n# the preprocessing differs from the original code, originally it was resize + crop\n# but we lose info while cropping, so here we use only resize to 224\npreprocess = transforms.Compose([\n    transforms.Resize(126),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n])\n\nbatch_size = 64\nsuffix = 'image_embs_dinov2_vitg14_reg'\ntrain_image_embeddings = get_image_embeddings_dino(model, preprocess, batch_size, train)\nnp.save(f'train_{suffix}', np.array(train_image_embeddings))\nval_image_embeddings = get_image_embeddings_dino(model, preprocess, batch_size, val)\nnp.save(f'val_{suffix}', np.array(val_image_embeddings))\ntest_image_embeddings = get_image_embeddings_dino(model, preprocess, batch_size, test)\nnp.save(f'test_{suffix}', np.array(test_image_embeddings))","metadata":{"execution":{"iopub.status.busy":"2024-08-11T05:57:03.822692Z","iopub.execute_input":"2024-08-11T05:57:03.823387Z"},"trusted":true},"execution_count":null,"outputs":[]}]}